---
title: 'Statistical Methods for Discrete Response, Time Series, and Panel Data (W271): Lab 2'
output: 'pdf_document'  
classoption: landscape
---

# The Keeling Curve

In the 1950s, the geochemist Charles David Keeling observed a seasonal pattern in the amount of carbon dioxide present in air samples collected over the course of several years. He was able to attribute this pattern to the variation in global rates of photosynthesis throughout the year, caused by the difference in land area and vegetation cover between the Earth's northern and southern hemispheres. 

In 1958 Keeling began continuous monitoring of atmospheric carbon dioxide concentrations from the Mauna Loa Observatory in Hawaii and soon observed a trend increase carbon dioxide levels in addition to the seasonal cycle. He was able to attribute this trend increase to growth in global rates of fossil fuel combustion. This trend has continued to the present, and is known as the "Keeling Curve."

```{r load packages, echo = FALSE, message = FALSE}
library(tidyverse)
library(ggplot2)
library(feasts)
library(tsibble)

library(latex2exp)
library(patchwork)
library(fable)
library(forecast)
library(tseries) # for adf.test
library(stargazer)

theme_set(theme_minimal())
knitr::opts_chunk$set(dpi=1000)
```

```{r plot the keeling curve, echo = FALSE}
tsibble::as_tsibble(co2) %>%
  ggplot() + 
  aes(x=index, y=value) + 
  geom_line(color = 'steelblue') +
  labs(
    title = TeX(r'(Monthly Mean $CO_2$)'),
    subtitle = 'The "Keeling Curve"',
    x = 'Month and Year',
    y = TeX(r'($CO_2$ parts per million)')
  )
```
\newpage

# Your Assignment 

Your goal in this assignment is to produce a comprehensive analysis of the Mona Loa CO2 data that you will be read by an interested, supervising data scientist. Rather than this being a final report, you might think of this as being a contribution to your laboratory. You and your group have been initially charged with the task of investigating the trends of global CO2, and told that if you find "anything interesting" that the team may invest more resources into assessing the question. 

Because this is the scenario that you are responding to: 

1. Your writing needs to be clear, well-reasoned, and concise. Your peers will be reading this, and you have a reputation to maintain.
2. Decisions that you make for your analysis need also be clear and well-reasoned. While the main narrative of your deliverable might only present the modeling choices that you determine are the most appropriate, there might exist supporting materials that examine what the consequences of other choices would be. As a concrete example, if you determine that a series is an AR(1) process your main analysis might provide the results of the critical test that led you to that determination and the results of the rest of the analysis under AR(1) modeling choices. However, in an appendix or separate document that is linked in your main report, you might show what a MA model would have meant for your results instead.
3. Your code and repository are a part of the deliverable. If you were to make a clear argument that this is a question worth pursuing, but then when the team turned to continue the work they found a repository that was a jumble of coding idioms, version-ed or outdated files, and skeletons it would be a disappointment.

# Report from the Point of View of 1997 

For the first part of this task, suspend reality for a short period of time and conduct your analysis from the point of view of a data scientist doing their work in the early months of 1998. Do this by using data that is included in _every_ R implementation, the `co2` dataset. This dataset is lazily loaded with every R instance, and is stored in an object called `co2`. 

```{r}
co2 <- as_tsibble(co2) %>% filter(lubridate::year(index)<1998)

```


## (3 points) Task 0a: Introduction 

Introduce the question to your audience. Suppose that they _could_ be interested in the question, but they don't have a deep background in the area. What is the question that you are addressing, why is it worth addressing, and what are you going to find at the completion of your analysis. Here are a few resource that you might use to start this motivation. 

- [Wikipedia](https://en.wikipedia.org/wiki/Keeling_Curve)
- [First Publication](./background/keeling_tellus_1960.pdf)
- [Autobiography of Keeling](./background/keeling_annual_review.pdf)

## (3 points) Task 1a: CO2 data
Conduct a comprehensive Exploratory Data Analysis on the `co2` series. This should include (without being limited to) a [description of how, where and why ](https://gml.noaa.gov/ccgg/about/co2_measurements.html) the data is generated, a thorough investigation of the trend, seasonal and irregular elements. Trends both in levels and growth rates should be discussed (consider expressing longer-run growth rates as annualized averages).

What you report in the deliverable should not be your own process of discovery, but rather a guided discussion that you have constructed so that your audience can come to an understanding as succinctly and successfully as possible. This means that figures should be thoughtfully constructed and what you learn from them should be discussed in text; to the extent that there is _any_ raw output from your analysis, you should intend for people to read and interpret it, and you should write your own interpretation as well. 

```{r}
p1 <- autoplot(co2) +geom_smooth(color="lightgrey")+
  ggtitle("Fig.1 Atmospheric CO2 concentration\n monthly average, parts per million (ppm) ") +
  xlab(NULL) + ylab(NULL)
p2 <- co2 %>% index_by(year = lubridate::year(index)) %>%
  summarise(annual_avg = mean(value)) %>%
  mutate(annual_growth = (annual_avg / lag(annual_avg, 1) - 1) * 100) %>%
  autoplot(.vars = annual_growth) +
  xlab(NULL) + ylab(NULL) +
  ggtitle("Fig.2 Annual growth rate of\n concentration, %")
p3 <- gg_season(co2) +
  xlab(NULL) + ylab(NULL) +
  ggtitle("Fig.3 Seasonal plot of CO2 concentration")
p4 <- co2 %>% model(STL(value ~ trend(window = 120) + season(window = "periodic"),
                        robust = TRUE)) %>%
  components() %>% pull(remainder) %>%
  gghistogram() +
  ggtitle("Fig.4 Histogram of irregular\n component by STL")
# p3 <- ggAcf(co2$value)
# p4 <- ggPacf(co2$value)
(p1 | p2) / (p3 | p4)
```

The data measures the monthly average atmospheric CO2 concentration from 1959 to 1997, expressed in parts per million (ppm). It was initially collected by a infrared gas analyzer installed at Mauna Loa in Hawaii, which was one of the four analyzers installed by Keeling to evaluate whether there was a persistent increase in CO2 concentration. 

Fig.1 shows a clear long-term upward trend, which is confirmed by Fig.2 where the growth rate for each year is above zero. Fig.2 also suggests the average growth rate after 1970 is higher than that before 1970, although there's no evidence of accelerating growth.

Another feature of the data is its robust seasonal pattern, with peak in May and bottom in October almost every year (see Fig.3). Keeling believes it was the result of the activity of land plants.

Fig.4 is the histogram of the remaining or irregular components after removing the trend and the seasonal components from the data with STL^[Cleveland, R. B., Cleveland, W. S., McRae, J. E., & Terpenning, I. J. (1990). STL: A seasonal-trend decomposition procedure based on loess. Journal of Official Statistics, 6(1), 3â€“33.]. It looks like a normal distribution without obvious outliers.  


```{r}
co2 %>%
  features(value, unitroot_kpss)
co2 %>%
  mutate(d_value=difference(value)) %>%
  features(d_value, unitroot_kpss)
co2 %>%
  features(value, unitroot_ndiffs)
co2 %>% gg_tsdisplay(difference(value), plot_type="partial") +labs(subtitle = "Differenced Co2")
```


## (3 points) Task 2a: Linear time trend model

Fit a linear time trend model to the `co2` series, and examine the characteristics of the residuals. Compare this to a quadratic time trend model. Discuss whether a logarithmic transformation of the data would be appropriate. Fit a polynomial time trend model that incorporates seasonal dummy variables, and use this model to generate forecasts to the year 2020. 
```{r}
linear_trend_model<-co2 %>% model(TSLM(value~trend()))
```
Since the long term trend of the $CO_2$ data looks linear and the variation around the trend seems stable, a log transformation of the data is not necessary (which is also supported by the stable and symmetric residuals in Fig.5) and we can fit the original data with a linear time trend model as:

\begin{equation}
\label{eq:one}
\text{CO}_{2} = \beta_0 + \beta_1t + \epsilon_{t}
\end{equation} 

, which gives the parameters as:

\begin{equation}
\label{eq:two}
\text{CO}_{2} = 311.5 + 0.11*t + \epsilon_{t}
\end{equation}

This linear trend model implies that the $CO_2$ concentration increased 0.11/month on average during 1959 to 1997. However, the residuals plots in Fig.5 suggest this simple linear trend model is not adequate since: 1) the mean of the residual forms a "U" shape along time, suggesting a quadratic or higher order polynomial trend model may be better; 2) the ACF plots indicates strong seasonal patterns exists in the residuals, suggesting seasonal dummy variables should be included in the model.

```{r}
gg_tsresiduals(linear_trend_model) + ggtitle("Fig.5 Residual plot of the linear trend model")

```



```{r}
co2_copy <- co2 %>% append_row(600) %>%
  mutate(
    num_index = time(index),
    num_index_qudratic = num_index ^ 2,
    num_index_cubic = num_index ^ 3,
  )
for (i in 1:11) {
  name =
    co2_copy <-
    co2_copy %>% mutate("month_{i}" := ifelse(lubridate::month(index) == i, 1, 0))
}
co2_training = co2_copy %>% filter(lubridate::year(index) < 1991)
co2_valid = co2_copy %>% filter(lubridate::year(index) < 1998, lubridate::year(index) >= 1991)
co2_forecast = co2_copy %>% filter(lubridate::year(index) >= 1998)

# stargazer(model_linear,model_quadratic,model_cubic,type="text",
#           add.lines=list(c("AIC", round(AIC(model_linear),1), round(AIC(model_quadratic),1), round(AIC(model_cubic),1)),
#                          c("BIC", round(BIC(model_linear),1), round(BIC(model_quadratic),1), round(BIC(model_cubic),1))))
dummy_name=paste0("month_",1:11,collapse = "+")
fit <- co2_training |>
  model(
    model_linear = TSLM(as.formula(paste0("value ~ num_index   +",dummy_name))),
    model_quadratic = TSLM(as.formula(paste0("value ~ num_index  + num_index_qudratic +",dummy_name))),
    model_cubic = TSLM(as.formula(paste0("value ~ num_index + num_index_qudratic + num_index_cubic +",dummy_name)))
  )
report(fit)
vd <- forecast(fit,new_data = co2_valid)
co2_training %>% filter(index>=yearmonth("1985M01")) %>% autoplot(value,PI = FALSE)+autolayer(vd)+autolayer(co2_valid)
fabletools::accuracy(vd,co2_valid)
# although the result above prefer the cubic model, I would suggest a linear or quadratic one.
final_model_poly <- co2_copy %>% filter(lubridate::year(index)<1998) %>%
  model(TSLM(as.formula(paste0("value ~ num_index + ",dummy_name))))
fc_linear <- final_model_poly %>% forecast(co2_forecast)
co2_copy %>% filter(lubridate::year(index)<1998) %>% autoplot(value)+autolayer(fc_linear)

```



## (3 points) Task 3a: ARIMA times series model 

We will use the Box Jenkins process to find the best ARIMA model via the following steps:

- Determine the appropriate model from eda
- find the best parameters
- examine the residuals using dianostic plots and statistical tests.

From the initial plots, we saw visual evidence of autoregressive and seasonal components. The ACF plots showed long slow decay of positive lag correlation, evidence of differencing. There is evidence of seasonality as well. We expect a seasonal arima model (SARIMA) with differencing to be best. 

In this section, we fit the best SARIMA model and analyze the results. Simplicity is a desirable property in data science models to help explain the relationship between variables. We choose BIC as our information criteria because it penalizes complex models more than AIC or AICc and therefore selects more simple models with fewer parameters as the best ones. Lower BIC scores are better. 

```{r swap between denny and mingxi, echo=FALSE}
df <- co2
# df <- tsibble::as_tsibble(co2) %>%
#   filter(index < lubridate::ymd('1998-01-01'))
```
```{r search for best ARIMA model}
model.bic <-df %>%
  model(ARIMA(value ~ 0:1 + pdq(0:8,0:2,0:8) + PDQ(0:12,0:4,0:12), ic="bic", stepwise=F, greedy=F))

model.bic %>%
  report()

```
After searching over seasonal and non seasonal P,D, and Q variables, the best model was an ARIMA(0,1,1)(1,1,2)[12] model with BIC score of 201.78. Next, we conclude the Box Jenkins process to evaluate the model via diagnostic plots and statistical tests.

 
```{r diagnostic plots}
x <- model.bic %>% augment() # tsibble
residuals <- x$.resid # vector

par(mfrow=c(2,2))
plot(residuals)
acf(residuals)
pacf(residuals)
hist(residuals)
```

The residual plots show that the SARIMA model was effective, with the residuals looking like stationary white noise. The time series has a mean of 0 with about constant variance, the ACF plot shows no autocorrelation beyond the initial lag value. The PACF plot appears to have a significant peak around the 3rd lag term, but this may be due to randomness, as it is barely passing the dashed blue line.The histogram looks normally distributed at 0 with outliers creating a left tail. 

```{r test tests}
tsresid <- model.bic %>% augment() %>% select(.resid)
# adf test on residuals
dickey <- adf.test(tsresid$.resid, alternative = "stationary", k = 10)

# box-jund test
# null is data is independently distributed
resid.ts<-model.bic %>%
  augment() %>%
  select(.resid) %>%
  as.ts()
box_1 <- Box.test(resid.ts, lag = 1, type = "Ljung-Box")
box_10 <- Box.test(resid.ts, lag = 10, type = "Ljung-Box")

adf.test(tsresid$.resid, alternative = "stationary", k = 10)
Box.test(resid.ts, lag = 1, type = "Ljung-Box")
Box.test(resid.ts, lag = 10, type = "Ljung-Box")

# qqplot on residuals, histogram on residuals
p1 <- model.bic %>%
  augment() %>%
  select(.resid) %>% 
  ggplot() +
  geom_histogram(aes(x=.resid))

p2 <- model.bic %>%
  augment() %>%
  select(.resid) %>% 
  ggplot(aes(sample=.resid)) +
  geom_qq() + stat_qq_line()


p1/p2

```
We test the residuals for stationarity with the augmented dickey fuller test. The augmented dickey fuller test has the null hypothesis that the data is non stationary. With a p-value of `r dickey$p.value`, we reject the null hypothesis because there is enough evidence to say that the residuals are stationary.

The Box-Ljung test has the null hypothesis that the data presented is independently distributed. When presented with the residuals of the ARIMA model, the test had p-values of `r round(box_1$p.value,3)` and `r round(box_10$p.value,3)` for lag =1 and lag = 10 respectively. For both of those lags, we fail to reject the null hypothesis and conclude that the data is independently distributed.

Finally, we visually inspect the histogram of the residuals and the qq plot to see if the residuals appear normally distributed. The histogram has the guassian bell shaped curve with a few outliers. The qq plot shows that the data matches up with the normal distribution's quantiles. With these plots, we can confidently say that the residuals are visibly normally distributed. 

To conclude, both diagnostic plots and statistical tests show that the residuals are stationary with mean 0, constant variance, and no autoregression or seasonality. We forecast our model to the year 2022.

```{r}
model.bic %>% 
  forecast(h=(2022-1998)*12) %>%
autoplot(colour="cornflowerblue") +
autolayer(df, colour="black") +
  labs(y = "CO2 ppm",title = "CO2 levels from 1959 to 2022") +
  guides(colour = guide_legend(title = "Forecast"))
```

## (3 points) Task 4a: Forecast atmospheric CO2 growth 

```{r forecast}
fc_arima <- model.bic %>% forecast(h=1900)
fc <-fc_arima %>% mutate(upper=quantile(value,0.95),lower=quantile(value,0.05))
first_420 <- fc %>% filter(upper>=420)
first_420 <- min(first_420$index)
last_420 <- fc %>% filter(lower < 420)
last_420 <- max(last_420$index)

first_500 <- fc %>% filter(upper >= 500)
first_500 <- min(first_500$index)
last_500 <- fc %>% filter(lower<=500)
last_500 <- max(last_500$index)
```


Based on our model, the first time we could potentially see CO2 in 420 ppm is `r first_420` because that is when the upper 95% confidence interval (CI) of our model first reaches 420 ppm. The last time the model predicts we will see CO2 at 420 ppm is `r last_420`, which is based on the final time the lower 95% CI is below 420. 

The first time our model predicts the earth to reach 500 ppm CO2 on `r first_500`, which is when the 95% CI reaches 500 ppm.  The model's lower 95% CI never reaches 500, so there is no predicted final time. Below is the prediction of our model to the year 2100. Confidence intervals are shown fanning outward. The error of the predictions compounds overtime which expands the confidence intervals into a funnel shape. The farther out in time from the recorded data points, the less accurate the prediction.



```{r forecast plot}
model.bic %>%
  forecast(h=(2100-1998)*12) %>%
autoplot(colour="cornflowerblue") +
autolayer(df, colour="black") +
  labs(y = "CO2 ppm",title = "CO2 levels from 1959 to 2100") +
  guides(colour = guide_legend(title = "Forecast"))
```


Introduce the question to your audience. Suppose that they _could_ be interested 
in the question, but they don't have a deep background in the area. What is the 
question that you are addressing, why is it worth addressing, and what are you 
going to find at the completion of your analysis. Here are a few resource that 
you might use to start this motivation. 
â€‹
- [Wikipedia](https://en.wikipedia.org/wiki/Keeling_Curve)
- [First Publication](./background/keeling_tellus_1960.pdf)
- [Autobiography of Keeling](./background/keeling_annual_review.pdf)
â€‹
An alarming pace of increase in global temperatures is being observed. Climate change is the term used to describe this occurrence. It has been proven that anthropogenic emissions, or greenhouse gas emissions brought on by human activities like deforestation and fossil fuel burning, are to blame.The phenomenon of climate change has been thoroughly studied by eminent scientists, such as Charles David Keeling, who is credited with developing the Keeling Curve, a visual representation of the rise in carbon dioxide content in the Earth's atmosphere from 1959 to the present. The following research topics will be addressed using the same data that supports the Keeling Curve: 1) How have carbon dioxide emissions grown over time? 2) How much carbon dioxide emissions should be anticipated? We will use data on atmospheric CO2 concentrations gathered from Mauna Loa, Hawaii, in time-series analysis to respond to these study issues.   
â€‹
Our group will create a time-series model to demonstrate the growth in carbon emissions over the previous 38 years. What's more, we'll forecast how much carbon emissions will increase if nothing is done to lower anthropogenic emissions. These results are significant because rising carbon emissions have the potential to endanger our way of life by, among other things, causing higher temperatures, rising sea levels, and more frequent extreme weather events. We are hope that these results will be used to business strategies and initiatives that will lower our carbon footprint for the benefit of the environment. Additionally, we anticipate that these results will serve as motivation for employees at our organization to adopt actions that will lower their carbon emissions.
â€‹
## (3 points) Task 1a: CO2 data
Conduct a comprehensive Exploratory Data Analysis on the `co2` series. This should 
include (without being limited to) a [description of how, where and why ](https://gml.noaa.gov/ccgg/about/co2_measurements.html) the data is generated, 
a thorough investigation of the trend, seasonal and irregular elements. Trends 
both in levels and growth rates should be discussed (consider expressing longer-run 
growth rates as annualized averages).
â€‹
What you report in the deliverable should not be your own process of discovery, 
but rather a guided discussion that you have constructed so that your audience 
can come to an understanding as succinctly and successfully as possible. This 
means that figures should be thoughtfully constructed and what you learn from 
them should be discussed in text; to the extent that there is _any_ raw output 
from your analysis, you should intend for people to read and interpret it, and 
you should write your own interpretation as well. 
â€‹
## Exploratory Data Analysis
â€‹
â€‹
In this report, we are analyzing the monthly CO2 levels data, captured at 
Mauna Loa observatory between Jan 1959 and Dec 1997, presented as monthly 
mean in PPM units. The unit of the data is in "mole fraction", which according 
to the data source is "defined as the number of carbon dioxide molecules in a 
given number of molecules of air, after removal of water vapor. For example, 
413 parts per million (PPM) of CO2 means that in every one million molecules of 
(dry) air there are on average 413 CO2 molecules." 
According to the documentation, the air at Mauna Loa is considered to be 
representative of most of the northern hemisphere and potentially the globe as 
well, as the observatory is situated at an altitude of 3400 meters and surrounded 
by bare lava of the active volcano.
â€‹
Original data CO2 represents CO2 observational data for a particular year-month
combination between Jan 1959 and Dec 1997.
â€‹
In the following section, we are performing initial EDA to better understand the 
data, starting by analyzing the time series, histogram, auto-correlation function 
(ACF), and partial auto-correlation function (PACF) plots.
â€‹
```{r, echo=FALSE, fig.align='center', fig.height=4, fig.width=10}
co2 %>%
  as_tsibble() -> co2_ts
â€‹
# Getting the overall CO2 values
value <- co2_ts$value
â€‹
# Making four different plots for evaluation
par(mfrow=c(2,2), mar = c(4.1, 4.1, 3, 2))
plot(value, type = "l", col="#FF7F00", main="CO2 time series",
     xlab="Time", ylab="CO2 rate")
pacf(value, col="#DF0030", main="PACF of CO2")
acf(value, col="#9D38BD", main="ACF of CO2")
hist(value, main="CO2 Distribution",
     ylab = "Frequency", xlab="CO2")
```
*Observations*
The top-left graph shows the time series data from January 1959 to January 1997. The data shows a clear linear trend and an annual seasonal pattern. The linear trend means that the data is increasing over time, and the annual seasonal pattern means that the data is higher in some months of the year than others.
â€‹
The top-right graph shows the autocorrelation function (ACF) of the series. The ACF measures the correlation between a time series and its lagged values. The ACF shows spikes at lags of 12 months, which indicates that there is annual seasonality in the data. The ACF also shows a gradual decline, which indicates that there is a linear trend in the data.
â€‹
The bottom-left graph shows the partial autocorrelation function (PACF) of the series. The PACF is similar to the ACF, but it removes the effects of the intervening lags. The PACF also shows spikes at lags of 1, 2, 12, and 13. These spikes suggest that there is some seasonality in the series, but that the seasonality is not perfectly periodic.
â€‹
The PACF also shows a significant spike at lag 1, which indicates that there is a strong relationship between the current value of the series and its previous value. This is consistent with the linear trend that was observed in the ACF plot.
â€‹
The PACF's strong relevance at various lags may indicate that the time series is an ARMA process. An ARMA process is a type of stochastic process that is characterized by both autoregressive (AR) and moving average (MA) terms.
â€‹
The bottom-right graph shows the histogram of the data. The histogram shows that the data is not normally distributed. This is not ideal for asymptotic confidence interval estimation, which requires the residuals to be normally distributed.
â€‹
In summary, the time series data shows some seasonality and a linear trend. The PACF plot suggests that the time series may be an ARMA process. However, the data is not normally distributed, which is not ideal for asymptotic confidence interval estimation.
â€‹
â€‹
```{r echo = FALSE, message = FALSE, fig.width = 4, fig.height = 4}
plot.season <- ggseasonplot(x = co2, 
                        year.labels = TRUE, 
                        year.labels.left = TRUE) +
           ylab(TeX(r'($CO_2$ PPM)')) +
           ggtitle(TeX(r'(Monthly Mean Seasonal Plot for $CO_2$ PPM Level)'))
â€‹
plot.season
```
â€‹
â€‹
â€‹
â€‹
```{r echo = FALSE, message = FALSE, fig.width = 4, fig.height = 2}
plot.subseries <- ggsubseriesplot(x = co2) +
                  ylab(TeX(r'($CO_2$ PPM)')) +
                  ggtitle(TeX(r'(Monthly Mean  Plot for $CO_2$ PPM Level)'))
plot.subseries
```
```{r echo = FALSE, message = FALSE, fig.width = 4, fig.height = 2}
plot.box <- boxplot(co2 ~ cycle(co2))
plot.box
```
â€‹
*Observations*
The plots show that the CO2 level...